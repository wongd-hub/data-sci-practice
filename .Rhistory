group_by(CryoSleep) %>%
tally() %>%
mutate(dataset = 'Unmodified') %>%
bind_rows(
all_wrk_tmp %>%
filter(!is.na(CryoSleep)) %>%
group_by(CryoSleep) %>%
tally() %>%
mutate(dataset = 'Imputed')
) %>%
group_by(dataset) %>%
mutate(pct = n / sum(n)) %>%
ungroup() %>%
ggplot(aes(x = dataset, fill = CryoSleep, y = pct, label = percent(pct, 0.1))) +
geom_col(position = 'dodge') +
scale_y_continuous(labels = percent) +
labs(x = 'Dataset', y = 'Percent') +
geom_text(
position = position_dodge(width = .9),
vjust = -0.5,
size = 3
) +
theme(legend.position = 'bottom')
# 3bi. Luxury Amenities part 1 - CryoSleeping Individuals ----
# We can then infer that all luxury amenities will be 0 for people in CryoSleep - this will fix luxury amenities for 513 observations
all_cryo_sub %>% filter(CryoSleep & is.na(lux_spend)) %>% nrow()
# We now replace all NAs in the luxury amenities with 0 where CryoSleep is TRUE
all_wrk_tmp <- all_wrk_tmp %>%
mutate_at(
.vars = vars(ShoppingMall, VRDeck, FoodCourt, Spa, RoomService),
.funs = ~ ifelse(CryoSleep & is.na(.), 0, .)
)
# 3bii. Luxury Amenities part 2 - Non-CryoSleeping Individuals ----
# Viewing relationship to other variables
# all_wrk_tmp %>%
#   filter(!CryoSleep) %>%
#   mutate(lux_spend = ShoppingMall + VRDeck + FoodCourt + Spa + RoomService) %>%
#   ggplot(aes(y = lux_spend, x = Deck, fill = Side)) +
#   geom_boxplot() +
#   scale_y_continuous(labels = dollar)
# all_wrk_tmp %>%
#   filter(!CryoSleep) %>%
#   mutate(lux_spend = ShoppingMall + VRDeck + FoodCourt + Spa + RoomService) %>%
#   ggplot(aes(y = lux_spend, x = Destination, fill = HomePlanet)) +
#   geom_boxplot() +
#   scale_y_continuous(labels = dollar)
# all_wrk_tmp %>%
#   filter(!CryoSleep) %>%
#   mutate(lux_spend = ShoppingMall + VRDeck + FoodCourt + Spa + RoomService) %>%
#   ggplot(aes(y = lux_spend, x = as.factor(group_size))) +
#   geom_boxplot() +
#   scale_y_continuous(labels = dollar)
# all_wrk_tmp %>%
#   filter(!CryoSleep) %>%
#   mutate(lux_spend = ShoppingMall + VRDeck + FoodCourt + Spa + RoomService) %>%
#   mutate(Num = as.numeric(Num)) %>%
#   ggplot(aes(x = Num, y = lux_spend, colour = HomePlanet)) +
#   geom_point() +
#   scale_y_continuous(labels = dollar) +
#   scale_x_continuous(labels = comma)
#  Side note, HomePlanet appears to be related to room number a bit - Europa are all low ~<500 numbers
#   Amenities spend otherwise appears to be related to Destination, HomePlanet, Deck, attempt
#    imputation via random forest
all_wrk_noncryo <- all_wrk_tmp %>% filter(!CryoSleep | is.na(CryoSleep))
imput_lux_spend <- all_wrk_noncryo %>%
mutate(
HomePlanet_fct = factor(HomePlanet),
Destination_fct = factor(Destination),
Deck_fct = factor(Deck)
) %>%
select(HomePlanet_fct, Destination_fct, Deck_fct, ShoppingMall, VRDeck, FoodCourt, Spa, RoomService) %>%
# No imputation method for the first three variables, then random forests for the rest
mice(method = 'rf', seed = 24601, maxit = 1) %>%
complete() %>%
select(-ends_with('_fct'))
#  Compare imputed values vs. non-imputed (label rows with NAs with 'imputed' flag)
all_wrk_noncryo %>%
mutate(imputed = ifelse(
is.na(ShoppingMall + VRDeck + FoodCourt + Spa + RoomService),
TRUE, FALSE
)) %>%
select(-ShoppingMall, -VRDeck, -FoodCourt, -Spa, -RoomService) %>%
bind_cols(
imput_lux_spend %>%
select(ShoppingMall, VRDeck, FoodCourt, Spa, RoomService)
) %>%
select(imputed, ShoppingMall, VRDeck, FoodCourt, Spa, RoomService) %>%
gather(variable, value, -imputed) %>%
ggplot(aes(x = value, fill = imputed)) +
geom_histogram() +
facet_grid(rows = vars(variable), scale = 'free') +
scale_y_continuous(labels = comma) +
scale_x_continuous(labels = dollar) +
labs(x = 'Value', y = 'Count', fill = 'Imputed?')
#   Add imputed values back to dataset
all_wrk_tmp <- all_wrk_tmp %>%
# Filter out !CryoSleep and is.na(CryoSleep)
filter(CryoSleep) %>%
bind_rows(
all_wrk_noncryo %>%
select(-ShoppingMall, -VRDeck, -FoodCourt, -Spa, -RoomService) %>%
bind_cols(
imput_lux_spend %>%
select(ShoppingMall, VRDeck, FoodCourt, Spa, RoomService)
)
) %>%
mutate(lux_spend = ShoppingMall + VRDeck + FoodCourt + Spa + RoomService)
# Check distributions before and after
all_wrk %>%
select(ShoppingMall, VRDeck, FoodCourt, Spa, RoomService) %>%
mutate(dataset = 'Unmodified') %>%
bind_rows(
all_wrk_tmp %>%
select(ShoppingMall, VRDeck, FoodCourt, Spa, RoomService) %>%
mutate(dataset = 'Imputed')
) %>%
gather(variable, value, -dataset) %>%
ggplot(aes(x = value, fill = dataset)) +
geom_histogram(position = 'dodge') +
facet_grid(rows = vars(variable), scales = 'free') +
theme(legend.position = 'bottom') +
scale_x_continuous(limits = c(NA, 10000), labels = comma) +
scale_y_continuous(labels = comma) +
labs(x = 'Value', y = 'Count', fill = 'Dataset')
# 3b. Inferring HomePlanet using Deck & Room Number ----
# Determine whether HomePlanet is related to Deck
#  From this we can reasonably infer that if in Decks A, B, C, or T, your HomePlanet was Europa
#  Further, it appears that Deck G is only inhabited by people from Earth
all_wrk_tmp %>%
group_by(HomePlanet, Deck) %>%
tally() %>%
ggplot(aes(x = Deck, y = n, fill = HomePlanet)) +
geom_col(position = 'dodge')
# If passenger's Deck is A-C or T, fix HomePlanet to Europa
# If passenger's Deck is G, fix HomePlanet to Earth
all_wrk_tmp <- all_wrk_tmp %>%
mutate(
HomePlanet = case_when(
is.na(HomePlanet) & Deck %in% c('A', 'B', 'C', 'T') ~ 'Europa',
is.na(HomePlanet) & Deck == 'G' ~ 'Earth',
TRUE ~ HomePlanet
)
)
# Room number also seems related to HomePlanet
all_wrk_tmp %>%
mutate(Num = as.numeric(Num)) %>%
select(HomePlanet, Num) %>%
ggplot(aes(x = Num, fill = HomePlanet)) +
geom_histogram(position = 'dodge')
# Finally, those within the same Group will likely have the same HomePlanet
all_wrk_tmp %>%
distinct(GroupId, HomePlanet) %>%
filter(!is.na(HomePlanet)) %>%
group_by(GroupId) %>%
summarise(n = n(), .groups = 'drop') %>%
filter(n > 1) %>% nrow() == 0 # Confirmed
all_wrk_grp_homes <- all_wrk_tmp %>%
filter(!is.na(HomePlanet)) %>%
distinct(GroupId, HomePlanet) %>%
rename(HomePlanet_supplement = HomePlanet)
all_wrk_tmp <- all_wrk_tmp %>%
left_join(all_wrk_grp_homes, by = c("GroupId")) %>%
mutate(HomePlanet = coalesce(HomePlanet, HomePlanet_supplement), .keep = 'unused')
# For all else, impute HomePlanet using room number and deck using mice
imput_homeplanet <- all_wrk_tmp %>%
mutate(
HomePlanet_fct = factor(HomePlanet),
Deck_fct = factor(Deck),
Num_num = as.numeric(Num)
) %>%
select(HomePlanet_fct, Deck_fct, Num_num) %>%
mice(method = 'rf', seed = 24601, maxit = 1) %>%
complete() %>%
mutate(HomePlanet = as.character(HomePlanet_fct)) %>%
select(-ends_with('_fct'), -ends_with('_num'))
# Add these back to the dataset
all_wrk_tmp <- all_wrk_tmp %>%
select(-HomePlanet) %>%
bind_cols(imput_homeplanet)
# Check distribution of imputed
all_wrk %>%
group_by(HomePlanet) %>%
tally() %>%
mutate(pct = n / sum(n)) %>%
mutate(dataset = 'Unmodified') %>%
bind_rows(
all_wrk_tmp %>%
group_by(HomePlanet) %>%
tally() %>%
mutate(pct = n / sum(n)) %>%
mutate(dataset = 'Imputed')
) %>%
ggplot(aes(x = HomePlanet, y = pct, fill = dataset, label = percent(pct, 0.1))) +
geom_col(position = 'dodge') +
scale_y_continuous(labels = percent) +
theme(legend.position = 'bottom') +
labs(x = 'Home Planet', y = 'Percent', fill = 'Dataset') +
geom_text(
position = position_dodge(width = .9),
vjust = -0.5,
size = 3
)
# 3c. Inferring Deck using Group ----
# Check if there are any groups between test and training - there are none
all_wrk_tmp %>%
distinct(GroupId, dataset) %>%
group_by(GroupId) %>%
tally() %>%
ungroup() %>%
filter(n > 1) %>%
arrange_all()
# Hypothesis is that, although groups don't stay in the same room, they tend to stay
#  in decks that are adjacent to each other
# First, we check if everyone in the same group generally stay in the same room together
all_wrk_grps <- all_wrk_tmp %>%
filter(group_size > 1) %>%
select(GroupId, PassengerId, Deck, Num, Side) %>%
group_by(GroupId) %>%
mutate(count = n()) %>%
ungroup() %>%
arrange(GroupId, PassengerId)
# Checking to see if whole groups stay within the exact same room
#  Doesn't seem to occur - only pattern is larger groups are more likely to be spread across rooms
all_wrk_grps %>%
# All distinct combinations of GroupId and Cabin
distinct(GroupId, Deck, Num, Side) %>%
group_by(GroupId) %>%
tally() %>%
# Adding back group_size variable
left_join(all_grps, by = 'GroupId') %>%
mutate(n_grouping = ifelse(n == 1, 'one room', 'more than one room')) %>%
group_by(group_size, n_grouping) %>%
tally() %>%
ungroup() %>%
group_by(group_size) %>%
mutate(total = sum(n)) %>%
mutate(pct = n / total) %>%
ggplot(aes(x = group_size, y = pct, fill = n_grouping)) +
geom_col(position = 'dodge')
# Check for Deck closeness within groups
#  Assumption here is that decks are in alphabetical order on the ship and therefore are closer
#   together
deck_refactor <- tibble(
Deck = all_wrk_grps %>% filter(!is.na(Deck)) %>% distinct(Deck) %>% pull(Deck) %>% sort(),
DeckNum = c(1:(all_wrk_grps %>% filter(!is.na(Deck)) %>% distinct(Deck) %>% pull(Deck) %>% length()))
)
all_wrk_grps %>%
left_join(deck_refactor, by = 'Deck') %>%
group_by(GroupId) %>%
summarise(
range = max(DeckNum) - min(DeckNum),
.groups = 'drop'
) %>%
ggplot(aes(x = range)) +
geom_histogram() +
scale_y_continuous(labels = comma) +
labs(x = 'Deck Distance with Groups', y = 'Count')
# Any other relationships? VIP/spenders?
#  Slight relationship with number, tend to be smaller room numbers
all_wrk_tmp %>%
mutate(Num = as.numeric(Num)) %>%
ggplot(aes(x = Num, fill = VIP)) +
geom_histogram()
#  Not really any relationship with Decks
all_wrk_tmp %>%
group_by(Deck, VIP) %>%
tally() %>%
ungroup() %>%
ggplot(aes(y = n, x = Deck, fill = VIP)) +
geom_col()
# Any relationship with lux_spend? Not really
all_wrk_tmp %>%
ggplot(aes(x = lux_spend)) +
geom_histogram() +
facet_grid(rows = vars(Deck))
# Could perform some probability distribution stuff with the above distribution
# Imputing with mice and using GroupId doesn't really work though, we'll create a 'missing' bucket for now
all_wrk_tmp <- all_wrk_tmp %>%
mutate(Deck = ifelse(is.na(Deck), 'NA', Deck))
# 3d. Final Rollup ----
# Check for NAs in our features of interest
all_wrk_tmp %>%
select_at(vars(inf_value %>% filter(IV > 0.1) %>% .$Variable)) %>%
sapply(function(x) sum(is.na(x))) # All good
# Impute some further variables using mean in case we'll need them later
imput_age <- all_wrk_tmp %>%
select(-GroupId, -PassengerId, -Transported, -dataset, -Num, -ends_with('_name')) %>%
# Convert all character columns to factors
mutate_if(
is.character,
factor
) %>%
mice(method = 'rf', seed = 24601, maxit = 1) %>%
complete()
all_wrk_tmp <- all_wrk_tmp %>%
select(-Age) %>%
bind_cols(imput_age %>% select(Age))
imput_homeplanet <- all_wrk_tmp %>%
mutate(
HomePlanet_fct = factor(HomePlanet),
Deck_fct = factor(Deck),
Num_num = as.numeric(Num)
) %>%
select(HomePlanet_fct, Deck_fct, Num_num) %>%
mice(method = 'rf', seed = 24601, maxit = 1) %>%
complete() %>%
mutate(HomePlanet = as.character(HomePlanet_fct)) %>%
select(-ends_with('_fct'), -ends_with('_num'))
# Check that we haven't lost anyone
identical(
all_wrk %>% distinct(GroupId, PassengerId) %>% arrange_all(),
all_wrk_tmp %>% distinct(GroupId, PassengerId) %>% arrange_all()
) # True, all good.
# Finally, convert features of interest and response into factors unless they're numeric
all_wrk_final <- all_wrk_tmp %>%
mutate_at(
.vars = vars(Transported, CryoSleep, Deck, HomePlanet),
.funs = factor
)
## 4. Training ----
# 4a. Splitting into train/validate/test ----
# Split back into training and test sets
test_clean <- all_wrk_final %>% filter(dataset == 'test') %>% select(-dataset)
train_clean <- all_wrk_final %>% filter(dataset == 'train') %>% select(-dataset)
# Split train_clean dataset into training and validation sets - ensure people in the
#  same group are either in one or the other dataset
set.seed(24601)
train_val_split <- train_clean %>%
group_by(GroupId) %>%
# Get count of rows under each GroupId
tally() %>%
# Generate a random number for each row which will be compared against the
#  probability of being in the test set (70%).
mutate(rand = runif(nrow(.))) %>%
mutate(grouping = ifelse(rand < 0.7, 'train', 'validation'), .keep = 'unused')
# Confirm this preserves the ~70/30 split we've aimed for:
train_val_split %>%
group_by(grouping) %>%
summarise(n = sum(n), .groups = 'drop') %>%
mutate(pct = percent(n / sum(n), 0.1))
train_new <- train_clean %>% filter(GroupId %in% (train_val_split %>% filter(grouping == 'train') %>% .$GroupId))
valid_new <- train_clean %>% filter(GroupId %in% (train_val_split %>% filter(grouping == 'validation') %>% .$GroupId))
# 4b. Random Forest ----
# 4bi. Fitting process ----
set.seed(24601)
st_rf_mod <- randomForest(
as.formula(
paste('Transported', "~", c(inf_value %>% filter(IV > 0.1) %>% .$Variable, 'Age', 'gender') %>% paste(collapse = " + "))
),
data = train_new
)
# Plot OOB error
plot(st_rf_mod)
legend('topright', colnames(st_rf_mod$err.rate), col=1:3, fill=1:3)
# Plot variable importance
importance_output <- importance(st_rf_mod)
importance <- tibble(
var = row.names(importance_output),
imp = round(importance_output[ ,'MeanDecreaseGini'],2)
)
importance %>%
mutate(var = factor(var, levels = importance %>% arrange(imp) %>% .$var)) %>%
ggplot(aes(x = imp, y = var)) +
geom_col() +
labs(x = 'Importance', y = 'Variable')
# 4bii. Validation ----
predicted_values <- valid_new %>%
bind_cols(
Prediction = predict(st_rf_mod, valid_new)
)
# Confusion matrix and accuracy metrics
prediction_cm <- confusionMatrix(
predicted_values$Prediction, predicted_values$Transported)
prediction_cm$table
prediction_cm$byClass[['Sensitivity']]
prediction_cm$byClass[['Specificity']]
# AUC
performance(
prediction(
predict(st_rf_mod, type = "prob")[,2],
as.numeric(train_new$Transported) - 1
),
measure = "auc"
)@y.values[[1]] # 0.8586274
# Sub-groups
#  For now we'll look at the sub-group of VIPs, since there aren't many, we're not expecting the model
#  to have done particularly well here
prediction_vip_cm <- confusionMatrix(
(predicted_values %>% filter(VIP))$Prediction, (predicted_values %>% filter(VIP))$Transported
)
prediction_vip_cm$table
prediction_vip_cm$byClass[['Sensitivity']]
prediction_vip_cm$byClass[['Specificity']]
# 4c. XGBoost ----
# Note XGBoost only works with numeric vectors, we'll modify our three datasets so that factors are one-hot encoded
#  and logicals are 0-1 (only for the variables of interest though)
# Define a function pipeline to clean all three data-frames
xgb_prep <- function(data, iv_threshold = 0.1) {
# Bin Age and filter only for variables we're interested in
data_tmp <- data %>%
mutate(AgeGrp = cut(Age, breaks = 10 * c(-1:10))) %>%
select_at(vars(inf_value %>% filter(IV > iv_threshold) %>% .$Variable, AgeGrp, Transported, gender))
# Force logical to numeric
data_tmp <- data_tmp %>%
mutate_at(
.vars = vars(CryoSleep#, Transported
),
.funs = ~ as.numeric(.) - 1
) %>%
# Label needs to be a factor to show this is a classification problem
mutate(Transported = as.factor(Transported))
# One-hot encode Deck and HomePlanet
dummy_var_model <- dummyVars(~ Deck + HomePlanet + AgeGrp + gender, data = data_tmp)
# Add back to main dataset
data_tmp <- data_tmp %>%
select(-Deck, -HomePlanet, -AgeGrp, -gender) %>%
bind_cols(
predict(dummy_var_model, newdata = data_tmp)
)
return(data_tmp)
}
train_xgb <- xgb_prep(train_new)
valid_xgb <- xgb_prep(valid_new)
test_xgb <- xgb_prep(test_clean)
# 4ci. Fitting process ----
# First, define the controls we want to train with; we're choosing 10-fold cross-validation and a grid search
xgb_control <- trainControl(
method = "cv",
number = 5,
search = "grid"
)
# Next, listing the possible hyperparameters we'll train over
#  For hyperparameters not listed here, we'll use the default value
xgb_hyp_params <- expand.grid(
max_depth = c(3, 4, 5, 6), # Controls the max depth of each tree; higher values = more chance of overfitting
nrounds = c(1:15) * 50, # Number of trees to go through
eta = c(0.01, 0.1, 0.2), # Analogous to learning rate
gamma = c(0, 0.01, 0.1), # The minimum loss reduction required to split the next node
# Default values for remaining hyperparameters
subsample = c(0.5, 0.75, 1),
min_child_weight = 1,
colsample_bytree = 0.6
)
# Unregister any parallel workers
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
set.seed(24601)
# Training the model
st_xgb_mod <- train(
Transported ~ .,
data = train_xgb,
method = "xgbTree",
trControl = xgb_control,
tuneGrid = xgb_hyp_params
)
xgb.plot.importance(
xgb.importance(
colnames(train_xgb %>% select(-Transported)),
model = st_xgb_mod$finalModel
)
)
# 4cii. Validation ----
predicted_values_xgb <- valid_new %>%
bind_cols(
Prediction = predict(st_xgb_mod, valid_xgb)
)
# Confusion matrix and accuracy metrics
prediction_cm_xgb <- confusionMatrix(
predicted_values_xgb$Prediction, predicted_values_xgb$Transported
)
prediction_cm_xgb$table
prediction_cm_xgb$byClass[['Sensitivity']]
prediction_cm_xgb$byClass[['Specificity']]
# AUC
performance(
prediction(
predict(st_xgb_mod, train_xgb, type = "prob")[,2],
as.numeric(train_new$Transported) - 1
),
measure = "auc"
)@y.values[[1]] # 0.911246
# Sub-groups
#  For now we'll look at the sub-group of VIPs, since there aren't many, we're not expecting the model
#  to have done particularly well here
prediction_vip_cm_xgb <- confusionMatrix(
(predicted_values_xgb %>% filter(VIP))$Prediction, (predicted_values_xgb %>% filter(VIP))$Transported
)
prediction_vip_cm_xgb$table
prediction_vip_cm_xgb$byClass[['Sensitivity']]
prediction_vip_cm_xgb$byClass[['Specificity']]
performance(
prediction(
predict(st_rf_mod, type = "prob")[,2],
as.numeric(train_new$Transported) - 1
),
measure = "cost"
)
performance(
prediction(
predict(st_rf_mod, type = "prob")[,2],
as.numeric(train_new$Transported) - 1
),
measure = "cost"
)@cutoffs
st_rf_cutoff <- performance(
prediction(
predict(st_rf_mod, type = "prob")[,2],
as.numeric(train_new$Transported) - 1
),
measure = "cost"
)
st_rf_cutoff %>% str()
