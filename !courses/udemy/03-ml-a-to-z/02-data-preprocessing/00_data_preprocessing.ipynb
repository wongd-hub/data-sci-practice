{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a2a9d3",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef535e9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Refresher on object-oriented programming</b>\n",
    "    <ul>\n",
    "        <li>A class is a model or blueprint of something we want to build. </li>\n",
    "        <li>An object is an instance of a class, and follows the class' blueprint.</li>\n",
    "        <li>A method is a tool/function we can use on an object.</li>\n",
    "    </ul><br>\n",
    "    <b>Further thoughts</b>\n",
    "    <ul>\n",
    "        <li> It's probably best practice to perform all your module/library imports at the start of each script; however for the purposes of this course, imports will generally be done just before their contents are used. This is to illustrate exactly where each class/function we're using originates from.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd146878",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f71f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011cd40",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee694908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/Data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730e1f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['France', 44.0, 72000.0],\n",
       "        ['Spain', 27.0, 48000.0],\n",
       "        ['Germany', 30.0, 54000.0],\n",
       "        ['Spain', 38.0, 61000.0],\n",
       "        ['Germany', 40.0, nan],\n",
       "        ['France', 35.0, 58000.0],\n",
       "        ['Spain', nan, 52000.0],\n",
       "        ['France', 48.0, 79000.0],\n",
       "        ['Germany', 50.0, 83000.0],\n",
       "        ['France', 37.0, 67000.0]], dtype=object),\n",
       " array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values # Use .values since a lot of these functions only take NumPy arrays, not pandas DataFrames\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ba5b3",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data\n",
    "\n",
    "Missingness can cause issues in machine learning algorithms. There are a few strategies to handle missing values:\n",
    "* Ignoring the observation - which can be an okay method as long as the values are missing at random and your dataset is large enough.\n",
    "* Another is to impute the missing value. In this course, we'll start by filling in these values with the average of the column. We'll use `scikit-learn`'s `SimpleImputer` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdeaae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Instantiate imputer object\n",
    "imputer = SimpleImputer(\n",
    "    missing_values = np.nan, # We want to replace all np.nans\n",
    "    strategy       = 'mean'  # Replace with the mean\n",
    ")\n",
    "\n",
    "# Fit the imputer to the data - note this method only takes numeric columns\n",
    "imputer.fit(X[:, 1:3])\n",
    "\n",
    "# Apply transformation and assign the result to the original columns\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89ab8a",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data\n",
    "\n",
    "How do we encode categorical values in a way that machine learning algorithms will be able to work with them?\n",
    "* We could give them a sequence of numbers; e.g. 1: France, 2: Spain, so on - but the ML algorithm might then incorrectly assume that there is a numerical order in this variable.\n",
    "* The answer to this is one-hot encoding where $n - 1$ columns are created ($n$ being the unique number of categories in the variable) and a value's existence is indicated by a combination of 0s and 1s.\n",
    "    * Using $n$ columns is known as the ['dummy variable trap'](https://www.learndatasci.com/glossary/dummy-variable-trap/) in regression models, wherein the last column is perfectly multicollinear with the other combination of columns causing issues when interpreting coefficients.\n",
    "    * Note as a result of the above reasoning, this is only a problem for models where collinear features are an issue. If they are, we can make use of the `drop` argument in `OneHotEncoder()`. This isn't done in this course so we'll continue with the default value for `drop` (which is `None`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f3bb3",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a06f500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# First, create an object from the ColumnTransformer class\n",
    "ct = ColumnTransformer(\n",
    "    # Specify: kind of transformation (encoding), what kind of encoding, and the columns we want to encode\n",
    "    transformers = [('encoder', OneHotEncoder(), [0])], \n",
    "    # Specifies that we want to keep all other columns that we haven't encoded with this object (everything but Country)\n",
    "    remainder    = 'passthrough'\n",
    ")\n",
    "\n",
    "# Connect this to our dataset; \n",
    "#  There is a fit_transform method that handles both fitting and transforming for ColumnTransformer\n",
    "#  ct.fit_transform doesn't return a numpy array, and we need this to feed into our ML algorithms, so force to array\n",
    "X = np.array(ct.fit_transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ac8eb",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable\n",
    "\n",
    "Similarly, we'll need our dependent variable to be numeric. Since it's a binary (Yes/No) variable, converting this to 0s and 1s will suffice. We can use `LabelEncoder()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0414f963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y  = le.fit_transform(y) # NB the dependent variable does not need to be a numpy array - likely because it's one column\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18f260",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set\n",
    "\n",
    "This is an exercise to split the dataset into one set that you train your model on and another set that you'll then test that trained model on. This concept can be taken even further and you'll often also create a validation set.\n",
    "\n",
    "The reason for this is as follows:\n",
    "1. The fundamental issue that causes you to need to consider a training and test set to begin with is **overfitting**; i.e. fitting your model not only to the signal in your dataset, but also the noise and peculiarities that come with it. This would mean that your model performs very well when predicting values within your training set, but will fall over when faced with data that it hasn't seen before.\n",
    "2. To try to minimise the risk of this occurring, you split your dataset into a training and test set, then train your model on the training set whilst evaluating the model's performance on the test set (i.e. the data the model hasn't seen before).\n",
    "    * This is all well and good, but after a large amount of iterations, you will likely end up overfitting to the test set as well since you are using and measuring performance over the same set repeatedly.\n",
    "    * The generally accepted solution is to create a training, test, and validation set; where once you're confident in the model you've build using your training and test set, you then test it once again on your validation set to confirm that overfitting hasn't substantially impacted your model's ability to predict on data it hasn't seen before.\n",
    "\n",
    "\n",
    "* Note overfitting is a fundamental problem to avoid in all forms of predictive modelling, and there are many other techniques that aim to reduce the chance of this occurring (including regularization).\n",
    "\n",
    "`sklearn` provides us with a function to split our dataset into train and test sets; this function returns four datasets: train set of features, test set of features, train set of labels, and test set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75095b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de9dffd9",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling\n",
    "\n",
    "This consists of scaling all your features so that they all take values on the same scale. We do this to prevent one feature from dominating another during the model training process. Note though that this doesn't have to be done for every type of model; and also doesn't need to be done on dummy variables.\n",
    "\n",
    "Consider multiple linear regression that takes the form $y = b_0 + b_1x_1 + \\dots + b_nx_n$; if you have $x_i$ that takes large values, the coefficient for that feature will compensate by becoming smaller. This is what we're trying to avoid when we scale our features.\n",
    "\n",
    "### A note on sequencing\n",
    "You have to apply feature scaling **after** splitting the dataset into the training and test set. The simple reason for this is that the test set is meant to be a **brand new set** on which you evaluate your ML model. \n",
    "\n",
    "Performing feature scaling on the whole dataset before splitting it into train/test would actually cause information leakage since information from the training set is now present in the test set.\n",
    "\n",
    "As a product of this, we'll also need to use the mean and standard deviation (or min and max) of $X_{train}$ to scale $X_{test}$.\n",
    "\n",
    "### Types of feature scaling\n",
    "* Standardisation: $x_{stand} = \\frac{x - mean(x)}{standard\\;deviation(x)}$; this generally bounds features between -3 and +3 (assuming normally distributed?)\n",
    "* Normalisation: $x_{norm} = \\frac{x - min(x)}{max(x)-min(x)}$; this generally bounds features between 0 and 1.\n",
    "\n",
    "Which one is better? \n",
    "* Normalisation is recommended when you have a normal distribution in most of your features.\n",
    "* Standardisation works all the time. Hence, we'll go with this for the most part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ae9407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.0, 0.0, 1.0, -0.19159184384578545, -1.0781259408412425],\n",
       "        [0.0, 1.0, 0.0, -0.014117293757057777, -0.07013167641635372],\n",
       "        [1.0, 0.0, 0.0, 0.566708506533324, 0.633562432710455],\n",
       "        [0.0, 0.0, 1.0, -0.30453019390224867, -0.30786617274297867],\n",
       "        [0.0, 0.0, 1.0, -1.9018011447007988, -1.420463615551582],\n",
       "        [1.0, 0.0, 0.0, 1.1475343068237058, 1.232653363453549],\n",
       "        [0.0, 1.0, 0.0, 1.4379472069688968, 1.5749910381638885],\n",
       "        [1.0, 0.0, 0.0, -0.7401495441200351, -0.5646194287757332]],\n",
       "       dtype=object),\n",
       " array([[0.0, 1.0, 0.0, -1.4661817944830124, -0.9069571034860727],\n",
       "        [1.0, 0.0, 0.0, -0.44973664397484414, 0.2056403393225306]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate a StandardScaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit this object to X_train and transform, after removing dummy variables\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "\n",
    "# Use this fitted object to transform X_test as well (note, don't fit it on X_test)\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])\n",
    "\n",
    "X_train, X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
