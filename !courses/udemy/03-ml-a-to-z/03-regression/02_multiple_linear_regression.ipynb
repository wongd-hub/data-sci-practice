{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ed2255",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e18098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d80fb",
   "metadata": {},
   "source": [
    "## Dataset and business problem description\n",
    "\n",
    "This dataset contains information on 50 companies, including their expenses, profits, and which state they operate in. The challenge is to determine how each of the factors available contribute to the profit level of each company.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "### Assumptions of linear regression\n",
    "\n",
    "These assumptions need to be checked before you build your linear regression model to ensure that the resulting model is valid.\n",
    "\n",
    "1. Linearity: the relationship between the dependent and independent variable(s) is linear. If the relationship is not linear then you'll need to use a non-linear model.\n",
    "2. Homoscedasticity: the variance of the residuals do not depend on the value of the independent variable (i.e. variance remains the same throughout the dataset). If the residuals vary with X, then the model is not optimal since it isn't capturing all the predictive information in the data.\n",
    "3. Multivariate normality: residuals are normally distributed around the mean value.\n",
    "4. Independence of errors: observations are independent of one another. This can be tested with the Durbin Watson statistic.\n",
    "5. Lack of multicollinearity: when independent variable are correlated with each other, there will be issues in interpreting the coefficients of the model. This can be tested with the Variance Inflation Factor method.\n",
    "\n",
    "### Dummy variables & intuition\n",
    "\n",
    "The business problem in this lesson is to see if there is any correlation between spending on R&D, administration, or marketing, as well as the state the startup is operating in; on the profit that the startup is generating. How would you go about creating a model to understand the relationship between these variables and profit? We can use a multiple linear regression for this.\n",
    "\n",
    "Multiple linear regression is a generalisation of simple linear regression to include multiple independent variables: $y = b_0 + b_1x_1 + \\dots + b_Nx_N$. \n",
    "\n",
    "The equation coefficients have a similar interpretation to in simple linear regression, where:\n",
    "\n",
    "* $b_0$ is still the intercept; where the dependent variable is when the independent variables is at 0.\n",
    "* $b_n$ is the gradient of variable $x_n$; this describes the change in the dependent variable with every unit change in of $x_n$, holding all other independent variables constant.\n",
    "\n",
    "\n",
    "When adding categorical variables to the model, you'll need to turn them into dummy variables - being careful to not fall into the dummy variable trap. i.e. Use $n-1$ dummy variables when there are $n$ categories; the variable you leave out then becomes the 'default' value, and its effect will be included in the intercept.\n",
    "\n",
    "### Understanding the P-value - statistical significance\n",
    "\n",
    "The intuition behind hypothesis testing is that you have two alternate universes:\n",
    "\n",
    "* $H_0$: Your null hypothesis, the 'default' universe; and,\n",
    "* $H_1$: Your alternative hypothesis, the universe where the thing you're trying to prove is true.\n",
    "\n",
    "Put simply, the P-value is the probability of the results that you found occurring, given that we're in a universe where the null hypothesis is true. Statistical significance ($\\alpha$) is defined as a P-value threshold below which the event that occurred is so unlikely that you reject the null hypothesis. In other words you are $(1-\\alpha)$% sure that we don't live in the 'default' universe (although there's a $\\alpha$% chance that we do).\n",
    "\n",
    "### Building a model (step-by-step)\n",
    "\n",
    "We have a lot of independent variables; we'll need to decide which ones to keep and which to discard. Why would we want to narrow the variable list down?\n",
    "\n",
    "* Garbage in, garbage out: there's no guarantee that more variables = better model; especially if there turns out to be multicollinearity in your predictors.\n",
    "* Explainability: you'll want to be able to explain the effect that each variable has on the outputs, this is hard to do when you have a lot of useless variables.\n",
    "\n",
    "This course presents 5 methods of building models (stepwise regression refers to methods 2-4):\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>On the validity of these methods</b><br/> \n",
    "    Most of the methods this course presents for finding the 'ideal' model in regression are stepwise selection processes. There are <a href=\"https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df\">arguments against this method of selection</a> citing the reason that the F-test used to assess the fit of the regression model (which differs from the t-test used to test each individual coefficient) is designed only for use in performing one hypothesis test, not many in sequence. As a result of this violation, the following are true:\n",
    "    <ul>\n",
    "        <li>Standard errors are biased toward 0 (i.e. overly optimistic)</li>\n",
    "        <li>p-values are biased towards 0 (i.e. overly optimistic)</li>\n",
    "        <li>Parameter estimates biased away from 0</li>\n",
    "        <li>Models too complex</li>\n",
    "    </ul><br/>\n",
    "    Alternatives presented include the use of LASSO regression and Least Angle Regression to arrive at a subset of predictors that are useful and predictive.\n",
    "\n",
    "Back to the course material...\n",
    "</div>\n",
    "\n",
    "#### All-in\n",
    "\n",
    "Throw all your variables in. When would you do this?\n",
    "\n",
    "* You have prior knowledge that all available variables are useful predictors; or,\n",
    "* You're required (legislation/business rules) to put these variables in; or,\n",
    "* You're preparing for backward elimination.\n",
    "\n",
    "#### Backward elimination\n",
    "\n",
    "1. Select a significance level as a threshold with which to keep a variable in the model (generally $\\alpha=0.05$).\n",
    "2. Fit full model with all possible predictors.\n",
    "3. Look at the predictor with the highest P-value. If this P-value exceeds the threshold go to Step 4. Otherwise you are done.\n",
    "4. Remove the predictor and re-fit the model. Go back to Step 3.\n",
    "\n",
    "#### Forward selection\n",
    "\n",
    "A much more complex procedure.\n",
    "\n",
    "1. Select a significance level to enter the model (generally $\\alpha=0.05$).\n",
    "2. Fit regression models $y~x_n$ for all $n$ variables. Select the model with the lowest P-value.\n",
    "3. Keep the model with the variable selected in Step 2, and fit all possible models $y~x_1 + x_{n-1}$. Select the model where $x_n$ has the lowest P-value. If the P-value is smaller than $\\alpha$ then continue adding variables in this manner. Otherwise remove this latest variable where P > $\\alpha$ and you are left with your final model.\n",
    "\n",
    "#### Bidirectional elimination\n",
    "\n",
    "1. Select a significance level to enter the model, and a significance level to stay in the model.\n",
    "2. Perform the next step of forward selection (new variables must have P-value < significance level to enter).\n",
    "3. Perform all steps of backward elimination.\n",
    "4. Add another variable via forward selection and then run all steps of backward elimination again.\n",
    "5. Once you get to the point where no new variables can exit, and no old variables can enter, you are done.\n",
    "\n",
    "#### Score comparison\n",
    "\n",
    "Most resource intensive approach.\n",
    "\n",
    "1. Select a criterion of goodness-of-fit (for example, Akaike's Information Criterion).\n",
    "2. Construct all possible models ($2^N-1$ total combinations - 10 variables means 1,023 possible combinations).\n",
    "3. Select the model with the best criterion.\n",
    "\n",
    "We'll focus on using the backward elimination process in this course since it's generally the fastest of all of these methods.\n",
    "\n",
    "## Building the model\n",
    "### Importing the dataset\n",
    "\n",
    "Note there is no need to scale the features in regression since the coefficients for each variable will adjust based on the scale of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91550af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/50_Startups.csv')\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4b5858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R&D Spend          0\n",
       "Administration     0\n",
       "Marketing Spend    0\n",
       "State              0\n",
       "Profit             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NAs\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80479a",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d4d9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [0.0, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [0.0, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [0.0, 1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [0.0, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [1.0, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [0.0, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [0.0, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [1.0, 0.0, 0.0, 123334.88, 108679.17, 304981.62]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[(\n",
    "    'encoder', \n",
    "    OneHotEncoder(\n",
    "# Note the options below are redundant, since the Multiple Linear regression class will handle this automatically\n",
    "#         # Use this to make sure you know which var is dropped (dataset.State.unique().tolist())[0]\n",
    "#         categories = [dataset.State.unique().tolist()], \n",
    "#         # Use drop = 'first' to avoid dummy variable trap\n",
    "#         drop = 'first'\n",
    "    ), \n",
    "    [3])\n",
    "], remainder='passthrough')\n",
    "\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "X[1:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4546adf",
   "metadata": {
    "colab_type": "text",
    "id": "WemVnqgeA70k"
   },
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f424ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fa941",
   "metadata": {
    "colab_type": "text",
    "id": "k-McZVsQBINc"
   },
   "source": [
    "### Training the Multiple Linear Regression model on the Training set\n",
    "\n",
    "As noted earlier, this class handles the dummy variable trap automatically for you. It also performs feature selection for you. (See text below these code blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9735329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdde617e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.66383692e+01, -8.72645791e+02,  7.86007422e+02,  7.73467193e-01,\n",
       "        3.28845975e-02,  3.66100259e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa70994",
   "metadata": {},
   "source": [
    "There are 6 coefficients above (`.coef_` does not include the intercept), which implies that the model has used all variables provided, including all levels of the dummy variable. This runs counter to what was described in the course, that this class would take care of these issues automatically.\n",
    "\n",
    "When asked this question, teaching assistants noted that you don't need to always drop a level; [citing this link](https://datascience.stackexchange.com/questions/27957/why-do-we-need-to-discard-one-dummy-variable). No comment is made on the automatic variable selection claim.\n",
    "\n",
    "We'll continue with this model as this is what the course uses, but it would be wise to consider the dummy variable trap in the context of future problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0989382",
   "metadata": {
    "colab_type": "text",
    "id": "xNkXL1YQBiBT"
   },
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05eb011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.2 , 132582.28, 132447.74,  71976.1 , 178537.48, 116161.24,\n",
       "        67851.69,  98791.73, 113969.44, 167921.07])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cap printing at 2 dp\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bc74bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103015.2 , 103282.38],\n",
       "       [132582.28, 144259.4 ],\n",
       "       [132447.74, 146121.95],\n",
       "       [ 71976.1 ,  77798.83],\n",
       "       [178537.48, 191050.39],\n",
       "       [116161.24, 105008.31],\n",
       "       [ 67851.69,  81229.06],\n",
       "       [ 98791.73,  97483.56],\n",
       "       [113969.44, 110352.25],\n",
       "       [167921.07, 166187.94]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate predicted values against actuals\n",
    "np.concatenate(\n",
    "    (\n",
    "        y_pred.reshape(len(y_pred), 1), # Transposing array\n",
    "        y_test.reshape(len(y_test), 1)\n",
    "    ),\n",
    "    axis = 1 # Column-bind (0 is row-bind)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
